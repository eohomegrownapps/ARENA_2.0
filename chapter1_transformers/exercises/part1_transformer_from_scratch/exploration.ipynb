{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elyro2/arena/.arena-venv/lib/python3.8/site-packages/accelerate/utils/imports.py:178: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n",
      "/home/elyro2/arena/.arena-venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bde97f27c4c47f38d3480e57a77bcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43888d76fa794bd2aad981e7859faf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27142090b4c94c4583f956193ca40d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325123a477ce4ebbbd8a7dfda1706c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6223b903fc3c47299dc461432957f2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed20a16bc53447c85af34cc1d3402e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = (exercises_dir / \"part1_transformer_from_scratch\").resolve()\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow\n",
    "# import part1_transformer_from_scratch.solutions as solutions\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == '__main__'\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "In GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, <|endoftext|> with index 50256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-43.4317, -39.8365, -43.0660,  ..., -54.0877, -54.3452, -42.3645],\n",
       "         [-61.0647, -68.0792, -72.3322,  ..., -73.9381, -74.3353, -68.3130],\n",
       "         [-43.5787, -50.2103, -52.9083,  ..., -57.6991, -57.5098, -49.6761]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496,  2159]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_tokens(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,  2159]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_tokens(\"Hello World\", prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n",
      "['<|endoftext|>', '568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998, 47385,    13]], device='cuda:0')\n",
      "torch.Size([1, 7])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' transformer', '.']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing transformer.\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 50257])\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' I'\n"
     ]
    }
   ],
   "source": [
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "next_char = reference_gpt2.to_string(next_token)\n",
    "print(repr(next_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(reference_gpt2.to_str_tokens(tokens), next_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(314, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as\n",
      "import numpy as np\n",
      "import numpy as np import\n",
      "import numpy as np import time\n",
      "import numpy as np import time import\n",
      "import numpy as np import time import time\n",
      "import numpy as np import time import time.\n",
      "import numpy as np import time import time.time\n",
      "import numpy as np import time import time.time import\n",
      "import numpy as np import time import time.time import n\n",
      "import numpy as np import time import time.time import numpy\n"
     ]
    }
   ],
   "source": [
    "from einops import pack, rearrange\n",
    "\n",
    "prompt = \"import numpy as\"\n",
    "tokens = reference_gpt2.to_tokens(prompt).to(device)\n",
    "print(prompt)\n",
    "\n",
    "for i in range(10):\n",
    "    logits: Float[Tensor, \"1 seq_len n_toks\"] = reference_gpt2(tokens)\n",
    "    next_token = logits[0, -1].argmax(-1)\n",
    "    prompt += reference_gpt2.to_string(next_token)\n",
    "    print(prompt)\n",
    "    tokens, _ = pack([tokens, rearrange(next_token, \"-> () ()\")], \"b *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,    40,   716,   281,  4998, 47385,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[314]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(next_token, \"-> () ()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitions / notes\n",
    "\n",
    "- MLP as key-value pairs: each neuron fires if dot product of its 'key' with its input is >0, and returns a 'value' vector scaled to match the 'key'\n",
    "- MLP as knowledge storage\n",
    "- Unembedding: sometimes we set $W_E = W_U^T$ -- seems OK at first, until you realise that the direct path involving embedding and unembedding should approximate _bigram_ frequencies\n",
    "    - Note: should this necessarily hold?\n",
    "- Positional encoding\n",
    "    - We add rather than concatenate... residual stream is shared memory and under significant superposition [??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "position = 35\n",
    "d_model = 768\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "d_mlp = 3072 # (= 4 * d_model)\n",
    "d_head = 64 # (= d_model / n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(\"I am an amazing transformer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 7, 768)\n",
      "hook_pos_embed                 (1, 7, 768)\n",
      "blocks.0.hook_resid_pre        (1, 7, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.0.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.0.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 7, 768)\n",
      "blocks.0.hook_resid_mid        (1, 7, 768)\n",
      "blocks.0.hook_mlp_in           (1, 7, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 7, 768)\n",
      "blocks.0.hook_resid_post       (1, 7, 768)\n",
      "blocks.1.hook_resid_pre        (1, 7, 768)\n",
      "blocks.1.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.1.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.1.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.1.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.1.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.1.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.1.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.1.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.1.hook_attn_out         (1, 7, 768)\n",
      "blocks.1.hook_resid_mid        (1, 7, 768)\n",
      "blocks.1.hook_mlp_in           (1, 7, 768)\n",
      "blocks.1.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.1.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.1.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.1.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.1.hook_mlp_out          (1, 7, 768)\n",
      "blocks.1.hook_resid_post       (1, 7, 768)\n",
      "blocks.2.hook_resid_pre        (1, 7, 768)\n",
      "blocks.2.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.2.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.2.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.2.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.2.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.2.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.2.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.2.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.2.hook_attn_out         (1, 7, 768)\n",
      "blocks.2.hook_resid_mid        (1, 7, 768)\n",
      "blocks.2.hook_mlp_in           (1, 7, 768)\n",
      "blocks.2.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.2.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.2.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.2.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.2.hook_mlp_out          (1, 7, 768)\n",
      "blocks.2.hook_resid_post       (1, 7, 768)\n",
      "blocks.3.hook_resid_pre        (1, 7, 768)\n",
      "blocks.3.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.3.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.3.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.3.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.3.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.3.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.3.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.3.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.3.hook_attn_out         (1, 7, 768)\n",
      "blocks.3.hook_resid_mid        (1, 7, 768)\n",
      "blocks.3.hook_mlp_in           (1, 7, 768)\n",
      "blocks.3.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.3.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.3.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.3.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.3.hook_mlp_out          (1, 7, 768)\n",
      "blocks.3.hook_resid_post       (1, 7, 768)\n",
      "blocks.4.hook_resid_pre        (1, 7, 768)\n",
      "blocks.4.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.4.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.4.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.4.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.4.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.4.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.4.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.4.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.4.hook_attn_out         (1, 7, 768)\n",
      "blocks.4.hook_resid_mid        (1, 7, 768)\n",
      "blocks.4.hook_mlp_in           (1, 7, 768)\n",
      "blocks.4.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.4.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.4.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.4.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.4.hook_mlp_out          (1, 7, 768)\n",
      "blocks.4.hook_resid_post       (1, 7, 768)\n",
      "blocks.5.hook_resid_pre        (1, 7, 768)\n",
      "blocks.5.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.5.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.5.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.5.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.5.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.5.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.5.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.5.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.5.hook_attn_out         (1, 7, 768)\n",
      "blocks.5.hook_resid_mid        (1, 7, 768)\n",
      "blocks.5.hook_mlp_in           (1, 7, 768)\n",
      "blocks.5.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.5.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.5.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.5.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.5.hook_mlp_out          (1, 7, 768)\n",
      "blocks.5.hook_resid_post       (1, 7, 768)\n",
      "blocks.6.hook_resid_pre        (1, 7, 768)\n",
      "blocks.6.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.6.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.6.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.6.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.6.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.6.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.6.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.6.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.6.hook_attn_out         (1, 7, 768)\n",
      "blocks.6.hook_resid_mid        (1, 7, 768)\n",
      "blocks.6.hook_mlp_in           (1, 7, 768)\n",
      "blocks.6.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.6.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.6.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.6.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.6.hook_mlp_out          (1, 7, 768)\n",
      "blocks.6.hook_resid_post       (1, 7, 768)\n",
      "blocks.7.hook_resid_pre        (1, 7, 768)\n",
      "blocks.7.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.7.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.7.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.7.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.7.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.7.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.7.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.7.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.7.hook_attn_out         (1, 7, 768)\n",
      "blocks.7.hook_resid_mid        (1, 7, 768)\n",
      "blocks.7.hook_mlp_in           (1, 7, 768)\n",
      "blocks.7.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.7.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.7.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.7.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.7.hook_mlp_out          (1, 7, 768)\n",
      "blocks.7.hook_resid_post       (1, 7, 768)\n",
      "blocks.8.hook_resid_pre        (1, 7, 768)\n",
      "blocks.8.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.8.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.8.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.8.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.8.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.8.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.8.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.8.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.8.hook_attn_out         (1, 7, 768)\n",
      "blocks.8.hook_resid_mid        (1, 7, 768)\n",
      "blocks.8.hook_mlp_in           (1, 7, 768)\n",
      "blocks.8.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.8.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.8.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.8.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.8.hook_mlp_out          (1, 7, 768)\n",
      "blocks.8.hook_resid_post       (1, 7, 768)\n",
      "blocks.9.hook_resid_pre        (1, 7, 768)\n",
      "blocks.9.ln1.hook_scale        (1, 7, 1)\n",
      "blocks.9.ln1.hook_normalized   (1, 7, 768)\n",
      "blocks.9.attn.hook_q           (1, 7, 12, 64)\n",
      "blocks.9.attn.hook_k           (1, 7, 12, 64)\n",
      "blocks.9.attn.hook_v           (1, 7, 12, 64)\n",
      "blocks.9.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.9.attn.hook_pattern     (1, 12, 7, 7)\n",
      "blocks.9.attn.hook_z           (1, 7, 12, 64)\n",
      "blocks.9.hook_attn_out         (1, 7, 768)\n",
      "blocks.9.hook_resid_mid        (1, 7, 768)\n",
      "blocks.9.hook_mlp_in           (1, 7, 768)\n",
      "blocks.9.ln2.hook_scale        (1, 7, 1)\n",
      "blocks.9.ln2.hook_normalized   (1, 7, 768)\n",
      "blocks.9.mlp.hook_pre          (1, 7, 3072)\n",
      "blocks.9.mlp.hook_post         (1, 7, 3072)\n",
      "blocks.9.hook_mlp_out          (1, 7, 768)\n",
      "blocks.9.hook_resid_post       (1, 7, 768)\n",
      "blocks.10.hook_resid_pre       (1, 7, 768)\n",
      "blocks.10.ln1.hook_scale       (1, 7, 1)\n",
      "blocks.10.ln1.hook_normalized  (1, 7, 768)\n",
      "blocks.10.attn.hook_q          (1, 7, 12, 64)\n",
      "blocks.10.attn.hook_k          (1, 7, 12, 64)\n",
      "blocks.10.attn.hook_v          (1, 7, 12, 64)\n",
      "blocks.10.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.10.attn.hook_pattern    (1, 12, 7, 7)\n",
      "blocks.10.attn.hook_z          (1, 7, 12, 64)\n",
      "blocks.10.hook_attn_out        (1, 7, 768)\n",
      "blocks.10.hook_resid_mid       (1, 7, 768)\n",
      "blocks.10.hook_mlp_in          (1, 7, 768)\n",
      "blocks.10.ln2.hook_scale       (1, 7, 1)\n",
      "blocks.10.ln2.hook_normalized  (1, 7, 768)\n",
      "blocks.10.mlp.hook_pre         (1, 7, 3072)\n",
      "blocks.10.mlp.hook_post        (1, 7, 3072)\n",
      "blocks.10.hook_mlp_out         (1, 7, 768)\n",
      "blocks.10.hook_resid_post      (1, 7, 768)\n",
      "blocks.11.hook_resid_pre       (1, 7, 768)\n",
      "blocks.11.ln1.hook_scale       (1, 7, 1)\n",
      "blocks.11.ln1.hook_normalized  (1, 7, 768)\n",
      "blocks.11.attn.hook_q          (1, 7, 12, 64)\n",
      "blocks.11.attn.hook_k          (1, 7, 12, 64)\n",
      "blocks.11.attn.hook_v          (1, 7, 12, 64)\n",
      "blocks.11.attn.hook_attn_scores (1, 12, 7, 7)\n",
      "blocks.11.attn.hook_pattern    (1, 12, 7, 7)\n",
      "blocks.11.attn.hook_z          (1, 7, 12, 64)\n",
      "blocks.11.hook_attn_out        (1, 7, 768)\n",
      "blocks.11.hook_resid_mid       (1, 7, 768)\n",
      "blocks.11.hook_mlp_in          (1, 7, 768)\n",
      "blocks.11.ln2.hook_scale       (1, 7, 1)\n",
      "blocks.11.ln2.hook_normalized  (1, 7, 768)\n",
      "blocks.11.mlp.hook_pre         (1, 7, 3072)\n",
      "blocks.11.mlp.hook_post        (1, 7, 3072)\n",
      "blocks.11.hook_mlp_out         (1, 7, 768)\n",
      "blocks.11.hook_resid_post      (1, 7, 768)\n",
      "ln_final.hook_scale            (1, 7, 1)\n",
      "ln_final.hook_normalized       (1, 7, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    print(f\"{activation_name:30}\", tuple(activation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E          (50257, 768)\n",
      "pos_embed.W_pos    (1024, 768)\n",
      "blocks.0.ln1.w     (768,)\n",
      "blocks.0.ln1.b     (768,)\n",
      "blocks.0.ln2.w     (768,)\n",
      "blocks.0.ln2.b     (768,)\n",
      "blocks.0.attn.W_Q  (12, 768, 64)\n",
      "blocks.0.attn.W_K  (12, 768, 64)\n",
      "blocks.0.attn.W_V  (12, 768, 64)\n",
      "blocks.0.attn.W_O  (12, 64, 768)\n",
      "blocks.0.attn.b_Q  (12, 64)\n",
      "blocks.0.attn.b_K  (12, 64)\n",
      "blocks.0.attn.b_V  (12, 64)\n",
      "blocks.0.attn.b_O  (768,)\n",
      "blocks.0.mlp.W_in  (768, 3072)\n",
      "blocks.0.mlp.b_in  (3072,)\n",
      "blocks.0.mlp.W_out (3072, 768)\n",
      "blocks.0.mlp.b_out (768,)\n",
      "ln_final.w         (768,)\n",
      "ln_final.b         (768,)\n",
      "unembed.W_U        (768, 50257)\n",
      "unembed.b_U        (50257,)\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(f\"{name:18} {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    try: reference_output = gpt2_layer(input)\n",
    "    except: reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 7, 768])\n",
      "Output shape: torch.Size([1, 7, 768])\n",
      "Reference output shape: torch.Size([1, 7, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from einops import reduce\n",
    "import torch\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(\n",
    "        self, residual: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Each d_model vector x normalised to mean 0 and variance 1\n",
    "        # Then we apply elementwise affine scaling\n",
    "        mean = reduce(residual, \"b n h -> b n ()\", reduction=\"mean\")\n",
    "        var = rearrange(torch.var(residual, dim=-1, unbiased=False), \"b n -> b n ()\")\n",
    "        return (residual - mean) / torch.sqrt(var + self.cfg.layer_norm_eps) * self.w + self.b\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 15])\n",
      "Output shape: torch.Size([1, 15, 768])\n",
      "Reference output shape: torch.Size([1, 15, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 15])\n",
      "Output shape: torch.Size([1, 15, 768])\n",
      "Reference output shape: torch.Size([1, 15, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from einops import repeat\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        B, N = tokens.shape\n",
    "        return repeat(self.W_pos[torch.arange(0,N)], \"n h -> b n h\", b=B)\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-30.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 7, 768])\n",
      "Output shape: torch.Size([1, 7, 768])\n",
      "Reference output shape: torch.Size([1, 7, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from einops import einsum\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Compute attn mask\n",
    "        queries: Float[Tensor, \"b q_pos n_h d_h\"] = (\n",
    "            einsum(\n",
    "                self.W_Q,\n",
    "                normalized_resid_pre,\n",
    "                \"n_h d_m d_h, b q_pos d_m -> b q_pos n_h d_h\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        keys: Float[Tensor, \"b k_pos n_h d_h\"] = (\n",
    "            einsum(\n",
    "                self.W_K,\n",
    "                normalized_resid_pre,\n",
    "                \"n_h d_m d_h, b k_pos d_m -> b k_pos n_h d_h\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        attn_scores: Float[Tensor, \"b n_h q_pos k_pos\"] = einsum(\n",
    "            queries, keys, \"b q_pos n_h d_h, b k_pos n_h d_h -> b n_h q_pos k_pos\"\n",
    "        )\n",
    "        scaled_masked_attn_scores = self.apply_causal_mask(\n",
    "            attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        )\n",
    "        attn_weights: Float[Tensor, \"b n_h q_pos k_pos\"] = torch.softmax(scaled_masked_attn_scores, dim=-1)\n",
    "\n",
    "        # Compute values\n",
    "        values: Float[Tensor, \"b v_pos n_h d_h\"] = (\n",
    "            einsum(\n",
    "                self.W_V,\n",
    "                normalized_resid_pre,\n",
    "                \"n_h d_m d_h, b q_pos d_m -> b q_pos n_h d_h\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "        weighted_sum: Float[Tensor, \"b v_pos n_h d_h\"] = einsum(\n",
    "            attn_weights, values, \"b n_h q_pos v_pos, b v_pos n_h d_h -> b q_pos n_h d_h\"\n",
    "        )\n",
    "        per_head_outputs: Float[Tensor, \"b v_pos n_h d_m\"] = einsum(\n",
    "            self.W_O, weighted_sum, \"n_h d_h d_m, b v_pos n_h d_h -> b v_pos n_h d_m\"\n",
    "        )\n",
    "        outputs: Float[Tensor, \"b v_pos d_m\"] = (\n",
    "            reduce(per_head_outputs, \"b v_pos n_h d_m -> b v_pos d_m\", reduction=\"sum\")\n",
    "            + self.b_O\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        b, n, q, k = attn_scores.shape\n",
    "        mask = torch.tril(torch.ones(q, k, device=attn_scores.device))\n",
    "        return mask * attn_scores + (1.0 - mask) * self.IGNORE\n",
    "\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7b213975-9ec6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.40.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7b213975-9ec6\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"I\", \" am\", \" an\", \" amazing\", \" transformer\", \".\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9679255485534668, 0.03207448124885559, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8024235367774963, 0.16839206218719482, 0.029184352606534958, 0.0, 0.0, 0.0, 0.0], [0.6959055662155151, 0.12269634008407593, 0.14588488638401031, 0.035513248294591904, 0.0, 0.0, 0.0], [0.5661024451255798, 0.14705193042755127, 0.08665256202220917, 0.11258415132761002, 0.0876089483499527, 0.0, 0.0], [0.3729434907436371, 0.10843680799007416, 0.09471546858549118, 0.06574743986129761, 0.10521901398897171, 0.25293782353401184, 0.0], [0.44880154728889465, 0.09826763719320297, 0.06507749855518341, 0.026181980967521667, 0.11423355340957642, 0.2005738466978073, 0.04686390981078148]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004189981264062226, 0.9995810389518738, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00013394761481322348, 0.00951184332370758, 0.9903541803359985, 0.0, 0.0, 0.0, 0.0], [0.0008606771007180214, 0.0026100408285856247, 0.015066802501678467, 0.9814624786376953, 0.0, 0.0, 0.0], [3.717061917996034e-05, 0.0006769601604901254, 0.0012693023309111595, 0.00021407907479442656, 0.9978025555610657, 0.0, 0.0], [0.000712872133590281, 0.003516094759106636, 0.0036592858377844095, 0.0008389435242861509, 0.0072732651606202126, 0.9839995503425598, 0.0], [0.0021695969626307487, 0.0021034099627286196, 0.0008538559195585549, 3.099489549640566e-05, 0.010538781993091106, 0.0004600910469889641, 0.9838432669639587]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9424744844436646, 0.05752559006214142, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8506749868392944, 0.09740971028804779, 0.05191527679562569, 0.0, 0.0, 0.0, 0.0], [0.7641512751579285, 0.10970381647348404, 0.07497040927410126, 0.05117453634738922, 0.0, 0.0, 0.0], [0.6748191118240356, 0.08416196703910828, 0.054702505469322205, 0.07581549137830734, 0.11050091683864594, 0.0, 0.0], [0.6548573970794678, 0.08950038999319077, 0.06112745404243469, 0.10716885328292847, 0.06692469120025635, 0.020421214401721954, 0.0], [0.44739651679992676, 0.2318466305732727, 0.09501756727695465, 0.034874461591243744, 0.06979372352361679, 0.019005298614501953, 0.102065809071064]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1077800765633583, 0.8922199010848999, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011847442016005516, 0.5536141991615295, 0.43453842401504517, 0.0, 0.0, 0.0, 0.0], [0.06322263926267624, 0.021719152107834816, 0.0942729040980339, 0.8207852840423584, 0.0, 0.0, 0.0], [0.005095744971185923, 0.0005348753184080124, 0.0010105484398081899, 0.0046575660817325115, 0.9887012243270874, 0.0, 0.0], [0.0003563790232874453, 4.0360891944146715e-06, 7.24779147276422e-06, 3.1859886803431436e-05, 0.0018006410682573915, 0.997799813747406, 0.0], [0.014004446566104889, 0.0029278143774718046, 0.0033891601487994194, 0.009213365614414215, 0.09293679893016815, 0.02352479286491871, 0.8540035486221313]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9238101243972778, 0.07618984580039978, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37589243054389954, 0.10050592571496964, 0.5236016511917114, 0.0, 0.0, 0.0, 0.0], [0.41213729977607727, 0.08231587707996368, 0.1808476448059082, 0.32469913363456726, 0.0, 0.0, 0.0], [0.14052285254001617, 0.005812315735965967, 0.02827638015151024, 0.05733034759759903, 0.7680581212043762, 0.0, 0.0], [0.100265733897686, 0.00570490350946784, 0.008641439490020275, 0.00735876988619566, 0.059429239481687546, 0.8185999393463135, 0.0], [0.08224880695343018, 0.04139508679509163, 0.03180205076932907, 0.07347526401281357, 0.28753477334976196, 0.09485628455877304, 0.38868770003318787]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19326065480709076, 0.806739330291748, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09132383018732071, 0.0022218849044293165, 0.9064542651176453, 0.0, 0.0, 0.0, 0.0], [0.07127513736486435, 0.010394586250185966, 0.014608724974095821, 0.9037215113639832, 0.0, 0.0, 0.0], [0.007525218650698662, 8.942795102484524e-05, 7.544153049821034e-05, 3.4135894111386733e-06, 0.9923064112663269, 0.0, 0.0], [0.0025997429620474577, 5.843023700435879e-06, 1.044487453327747e-05, 4.1645202486506605e-07, 3.6627934605348855e-05, 0.9973468780517578, 0.0], [0.03889661282300949, 0.0011462486581876874, 0.0005508657777681947, 0.0001894300221465528, 0.012243179604411125, 0.0008620002772659063, 0.9461116194725037]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360752105712891, 0.06392483413219452, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8446658253669739, 0.06946855783462524, 0.0858655795454979, 0.0, 0.0, 0.0, 0.0], [0.6426600217819214, 0.1298869252204895, 0.17117173969745636, 0.05628127232193947, 0.0, 0.0, 0.0], [0.5074958801269531, 0.07786272466182709, 0.0726853460073471, 0.10711853206157684, 0.23483751714229584, 0.0, 0.0], [0.5321598052978516, 0.08092120289802551, 0.06559041887521744, 0.11006737500429153, 0.08751308917999268, 0.12374812364578247, 0.0], [0.3233777582645416, 0.04574260860681534, 0.05953827500343323, 0.039298463612794876, 0.275970458984375, 0.21313977241516113, 0.042932670563459396]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780336022377014, 0.021966375410556793, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5068984627723694, 0.42791497707366943, 0.06518658250570297, 0.0, 0.0, 0.0, 0.0], [0.34595075249671936, 0.23385104537010193, 0.19138787686824799, 0.22881032526493073, 0.0, 0.0, 0.0], [0.2785970866680145, 0.17525729537010193, 0.11037091165781021, 0.2518095374107361, 0.18396516144275665, 0.0, 0.0], [0.24355025589466095, 0.05037883669137955, 0.05468335747718811, 0.22595743834972382, 0.3045573830604553, 0.12087276577949524, 0.0], [0.09816339612007141, 0.08779838681221008, 0.07518380880355835, 0.13303788006305695, 0.19250334799289703, 0.06394527852535248, 0.3493678569793701]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8970723748207092, 0.10292765498161316, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7731762528419495, 0.17108607292175293, 0.055737756192684174, 0.0, 0.0, 0.0, 0.0], [0.2190985381603241, 0.09592850506305695, 0.15399746596813202, 0.5309754610061646, 0.0, 0.0, 0.0], [0.4932918846607208, 0.12421077489852905, 0.08378950506448746, 0.21044504642486572, 0.08826280385255814, 0.0, 0.0], [0.4699688255786896, 0.09084861725568771, 0.092851921916008, 0.04617578163743019, 0.06326503306627274, 0.2368898093700409, 0.0], [0.44727620482444763, 0.10623635351657867, 0.050856221467256546, 0.14603742957115173, 0.025409094989299774, 0.028171999379992485, 0.1960127204656601]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8803902268409729, 0.1196097880601883, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8001626133918762, 0.14422568678855896, 0.055611640214920044, 0.0, 0.0, 0.0, 0.0], [0.6471940875053406, 0.14701782166957855, 0.0793100893497467, 0.12647801637649536, 0.0, 0.0, 0.0], [0.6291000843048096, 0.13269942998886108, 0.08307387679815292, 0.10306607186794281, 0.052060507237911224, 0.0, 0.0], [0.6632711887359619, 0.0981065109372139, 0.06206610053777695, 0.08949960768222809, 0.07732616364955902, 0.009730454534292221, 0.0], [0.515776515007019, 0.13394522666931152, 0.06515839695930481, 0.08924729377031326, 0.07155614346265793, 0.043718595057725906, 0.08059780299663544]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533873081207275, 0.34661269187927246, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4451117515563965, 0.33376824855804443, 0.22112005949020386, 0.0, 0.0, 0.0, 0.0], [0.54598069190979, 0.1529107689857483, 0.0764545127749443, 0.22465400397777557, 0.0, 0.0, 0.0], [0.42720288038253784, 0.1412525475025177, 0.06143970414996147, 0.11979992687702179, 0.2503049075603485, 0.0, 0.0], [0.41781172156333923, 0.11764386296272278, 0.05689752846956253, 0.09480444341897964, 0.06836460530757904, 0.24447786808013916, 0.0], [0.32431575655937195, 0.13340629637241364, 0.06139463558793068, 0.07967764884233475, 0.12499963492155075, 0.03882276266813278, 0.23738326132297516]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8253123164176941, 0.1746877133846283, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7580502033233643, 0.13029679656028748, 0.1116529032588005, 0.0, 0.0, 0.0, 0.0], [0.7232240438461304, 0.0827442929148674, 0.07870831340551376, 0.11532339453697205, 0.0, 0.0, 0.0], [0.5594542622566223, 0.0869443342089653, 0.09115549921989441, 0.09393443912267685, 0.1685115247964859, 0.0, 0.0], [0.2734120488166809, 0.17145845293998718, 0.0807892382144928, 0.1144709512591362, 0.10775468498468399, 0.2521146535873413, 0.0], [0.40154963731765747, 0.04748968780040741, 0.05209226906299591, 0.08990596234798431, 0.15010744333267212, 0.1516994684934616, 0.1071554571390152]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x1515b3c3e1c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "from IPython.display import display\n",
    "\n",
    "html = cv.attention.attention_patterns(\n",
    "    tokens=reference_gpt2.to_str_tokens(reference_text), \n",
    "    attention=cache[\"pattern\", 0][0]\n",
    ")\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 7, 768])\n",
      "Output shape: torch.Size([1, 7, 768])\n",
      "Reference output shape: torch.Size([1, 7, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        hid: Float[Tensor, \"b p_n d_h\"] = gelu_new(\n",
    "            einsum(self.W_in, normalized_resid_mid, \"d_m d_h, b p_n d_m -> b p_n d_h\")\n",
    "            + self.b_in\n",
    "        )\n",
    "        result: Float[Tensor, \"b p_n d_m\"] = (\n",
    "            einsum(self.W_out, hid, \"d_h d_m, b p_n d_h -> b p_n d_m\")\n",
    "            + self.b_out\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 7, 768])\n",
      "Output shape: torch.Size([1, 7, 768])\n",
      "Reference output shape: torch.Size([1, 7, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        attn_out = self.attn(self.ln1(resid_pre))\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        mlp_out = self.mlp(self.ln2(resid_mid))\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 7, 768])\n",
      "Output shape: torch.Size([1, 7, 50257])\n",
      "Reference output shape: torch.Size([1, 7, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return einsum(self.W_U, normalized_resid_final, \"d_m d_v, b p d_m -> b p d_v\") + self.b_U\n",
    "\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 15])\n",
      "Output shape: torch.Size([1, 15, 50257])\n",
      "Reference output shape: torch.Size([1, 15, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        embeddings = self.embed(tokens)\n",
    "        pos_embs = self.pos_embed(tokens)\n",
    "        residual_stream = embeddings + pos_embs\n",
    "        for block in self.blocks:\n",
    "            residual_stream = block(residual_stream)\n",
    "        return self.unembed(self.ln_final(residual_stream))\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 1.7251\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.472316\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], \n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4bc5522e444c42b45ce89487d135cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arena-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
